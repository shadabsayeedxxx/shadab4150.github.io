# What is perplexity in NLP?

In English, the word 'perplexed' means 'puzzled' or 'confused'. 
Perplexity means inability to deal with or understand something complicated or unaccountable.

When a toddler or a baby speaks unintelligibly, we find ourselves 'perplexed'. 
Why ? because their spoken language does not comply with the grammar and construct of the language that we tend to understand and speak.

Now imagine you trained a machine learning NLP model on lots and lots of well written blogs and answers. 
Now the task is to evaluate how good a certain answer is (for, say, pushing it to the top of the feed). 

* Question : Among very many models that you trained, which model will you select for picking good blogs from bad ?

* Answer: You would pick that NLP model which is least "perplexed" when presented with a well written blog.

Thus, [**perplexity metric**](https://en.wikipedia.org/wiki/Perplexity) in NLP is a way to capture the degree of 'uncertainty' a model has in predicting (assigning probabilities to) some text. 
It is related to [**Shannon's Entropy**](https://en.wikipedia.org/wiki/Entropy_(information_theory)). 
Lower the entropy (uncertainty), lower the perplexity. 
If a model, which is trained on good blogs and is being evaluated on similarly looking good blogs, assigns higher probability, we say the model has lower perplexity than a model which assigns lower probability.

* Credits Quora User [**Anoop Deoras**](https://www.quora.com/profile/Anoop-Deoras)
